---
title: Prototypical Fine-tuning - Towards Robust Performance Under Varying Data Sizes 
suptitle: We propose prototypical fine-tuning, a novel prototypical framework for fine-tuning pretrained language models (LM), which automatically learns a bias to improve predictive performance for varying data sizes, especially low-resource settings.

# Summary for listings and search engines
summary: We propose prototypical fine-tuning, a novel prototypical framework for fine-tuning pretrained language models (LM), which automatically learns a bias to improve predictive performance for varying data sizes, especially low-resource settings.

# Link this post with a project
projects: []

# Date published
date: '2022-05-10T00:00:00Z'

# Date updated
lastmod: '2022-05-10T00:00:00Z'

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: false

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Our proposed PFit model: [**Unsplash**](https://unsplash.com/photos/CpkOjOcXdUY)'
  focal_point: ''
  placement: 2
  preview_only: false

authors:
  - admin

tags:
  - Academic

categories:
  - Research
  - NLP
---

Preprint.

## Authors

Yiqiao Jin<sup>1</sup>, Xiting Wang<sup>2</sup>, Yaru Hao<sup>2</sup>, Yizhou Sun<sup>1</sup>, Xing Xie<sup>2</sup>

<sup>1</sup>University of California, Los Angeles,
<sup>2</sup>Microsoft Research Asia

## Abstract

We move towards combining large parametric models with non-parametric prototypical networks. We propose prototypical fine-tuning, a novel prototypical framework for fine-tuning pretrained language models (LM), which automatically learns a bias to improve predictive performance for varying data sizes, especially low-resource settings. Our prototypical fine-tuning approach can automatically adjust the model capacity according to the data complexity and the model's inherent attributes. Moreover, we propose four principles for effective prototype fine-tuning towards the global optimum. Experimental results across various datasets show that our work achieves significant performance improvements under various low-resource settings, as well as comparable and usually better performances in high-resource scenarios.


s<!-- ## Get Started
{{< figure src="posts/getting-started/PFit.png" title="The template is mobile first with a responsive design to ensure that your site looks stunning on every device." >}}


## License

Copyright 2022-present [Yiqiao Jin](https://ahren09.github.io/).

Released under the [MIT](https://github.com/wowchemy/wowchemy-hugo-modules/blob/master/LICENSE.md) license.
